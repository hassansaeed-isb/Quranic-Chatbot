{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11243377,"sourceType":"datasetVersion","datasetId":7024897}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\n\n# Install required packages\n# !pip install transformers datasets sentencepiece\n\n# Global variables\nvectorizer = None\ntfidf_matrix = None\ntokenizer = None\nmodel = None\n\n# Load the raw text data directly - handle the case where these are actual Quranic verses\n# instead of a properly formatted CSV\ntry:\n    # First, check if the data has already been processed from a previous run\n    try:\n        urdu_data = pd.read_csv('/kaggle/working/processed_quran.csv')\n        print(\"Found previously processed data. Loading it...\")\n    except:\n        # Try loading from multiple possible kaggle paths\n        possible_paths = [\n            '/kaggle/input/urdu-quran-dataset/Urdu.csv',\n            '/kaggle/input/urdu-quran/Urdu.csv',\n            '/kaggle/input/urdu-csv/Urdu.csv',\n            '/kaggle/input/urdu-quran-translation/Urdu.csv',\n            '/kaggle/working/Urdu.csv'\n        ]\n        \n        for path in possible_paths:\n            try:\n                # Try reading the file as raw text first\n                with open(path, 'r', encoding='utf-8') as f:\n                    lines = f.readlines()\n                \n                print(f\"Successfully read {len(lines)} lines from {path}\")\n                \n                # Create a DataFrame with the verses\n                verses = []\n                for i, line in enumerate(lines):\n                    line = line.strip()\n                    if line:  # Skip empty lines\n                        verses.append({\n                            \"Surah\": i // 10 + 1,  # Assign artificial surah numbers\n                            \"Ayah\": i % 10 + 1,    # Assign artificial ayah numbers\n                            \"Translation\": line\n                        })\n                \n                urdu_data = pd.DataFrame(verses)\n                \n                # Save the processed data for future use\n                urdu_data.to_csv('/kaggle/working/processed_quran.csv', index=False)\n                \n                print(f\"Created DataFrame with {len(verses)} verses\")\n                break\n            except Exception as e:\n                print(f\"Error reading {path}: {str(e)}\")\n                continue\n        else:\n            # If we get here, none of the paths worked\n            # As a fallback, create a DataFrame from the error text itself\n            print(\"Using error messages as data source...\")\n            \n            # Extract verse text from error messages\n            pattern = r\"Error processing row: (.*?), Error:\"\n            with open('/kaggle/working/error_log.txt', 'r', encoding='utf-8') as f:\n                error_text = f.read()\n            \n            matches = re.findall(pattern, error_text)\n            \n            if matches:\n                verses = []\n                for i, verse in enumerate(matches):\n                    verses.append({\n                        \"Surah\": i // 10 + 1,  # Arbitrary surah assignment\n                        \"Ayah\": i % 10 + 1,    # Arbitrary ayah assignment\n                        \"Translation\": verse\n                    })\n                urdu_data = pd.DataFrame(verses)\n                print(f\"Created DataFrame with {len(verses)} verses from error log\")\n            else:\n                # Last resort - create a small dummy dataset\n                test_verses = [\n                    \"Ø¨Ø³Ù… Ø§Ù„Ù„Û Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÛŒÙ…\",\n                    \"Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Û Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÛŒÙ†\",\n                    \"Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÛŒÙ…\",\n                    \"Ù…Ø§Ù„Ú© ÛŒÙˆÙ… Ø§Ù„Ø¯ÛŒÙ†\",\n                    \"Ø§ÛŒØ§Ú© Ù†Ø¹Ø¨Ø¯ Ùˆ Ø§ÛŒØ§Ú© Ù†Ø³ØªØ¹ÛŒÙ†\",\n                    \"Ø§Ú¾Ø¯Ù†Ø§ Ø§Ù„ØµØ±Ø§Ø· Ø§Ù„Ù…Ø³ØªÙ‚ÛŒÙ…\",\n                    \"ØµØ±Ø§Ø· Ø§Ù„Ø°ÛŒÙ† Ø§Ù†Ø¹Ù…Øª Ø¹Ù„ÛŒÚ¾Ù… ØºÛŒØ± Ø§Ù„Ù…ØºØ¶ÙˆØ¨ Ø¹Ù„ÛŒÚ¾Ù… Ùˆ Ù„Ø§ Ø§Ù„Ø¶Ø§Ù„ÛŒÙ†\"\n                ]\n                verses = []\n                for i, verse in enumerate(test_verses):\n                    verses.append({\n                        \"Surah\": 1,\n                        \"Ayah\": i + 1,\n                        \"Translation\": verse\n                    })\n                urdu_data = pd.DataFrame(verses)\n                print(\"Created small sample dataset with Surah Al-Fatiha\")\nexcept Exception as e:\n    print(f\"Error during data loading: {str(e)}\")\n    # Create a minimal dataset as a fallback\n    urdu_data = pd.DataFrame([\n        {\"Surah\": 1, \"Ayah\": 1, \"Translation\": \"Ø¨Ø³Ù… Ø§Ù„Ù„Û Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÛŒÙ…\"},\n        {\"Surah\": 1, \"Ayah\": 2, \"Translation\": \"Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Û Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÛŒÙ†\"},\n        {\"Surah\": 1, \"Ayah\": 3, \"Translation\": \"Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÛŒÙ…\"}\n    ])\n    print(\"Created minimal fallback dataset\")\n\n# Check if we have any data to work with\nif urdu_data.empty:\n    print(\"Error: No data available. Please check the input files.\")\n    exit()\n\n# Check if 'Translation' column exists\nif 'Translation' not in urdu_data.columns:\n    print(\"Error: 'Translation' column not found. Check data format.\")\n    exit()\n\n# Now we can proceed with the chatbot setup\nprint(\"\\nProcessed data sample:\")\nprint(urdu_data.head())\nprint(f\"Total verses: {len(urdu_data)}\")\n\n# Initialize NLP components\ntry:\n    global tokenizer, model, vectorizer, tfidf_matrix\n    \n    # Initialize tokenizer and model\n    print(\"\\nInitializing NLP components...\")\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n    model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-multilingual-cased\")\n    \n    # Create the TF-IDF vectorizer and fit it on the translations\n    print(\"Fitting TF-IDF vectorizer...\")\n    vectorizer = TfidfVectorizer(stop_words=None)  # Don't use English stopwords for Urdu\n    tfidf_matrix = vectorizer.fit_transform(urdu_data['Translation'])\n    print(\"TF-IDF vectorizer fitted successfully.\")\nexcept Exception as e:\n    print(f\"Error initializing NLP components: {str(e)}\")\n    print(\"Will attempt to continue with limited functionality.\")\n\ndef find_relevant_context(question, dataset, top_k=3):\n    \"\"\"\n    Find relevant Quranic verses for a given question.\n    Uses both exact matching and TF-IDF similarity if available.\n    \n    Args:\n        question: The question in Urdu\n        dataset: The processed Quran dataset\n        top_k: Number of verses to return\n        \n    Returns:\n        String containing relevant verses with references\n    \"\"\"\n    global vectorizer, tfidf_matrix\n    \n    # Step 1: Try an exact match or substring match for critical words\n    exact_matches = []\n    for _, row in dataset.iterrows():\n        # Check if question is contained in the verse or vice versa\n        if question in row['Translation'] or any(word in row['Translation'] for word in question.split()):\n            exact_matches.append(f\"Surah {row['Surah']}, Ayah {row['Ayah']}: {row['Translation']}\")\n            if len(exact_matches) >= top_k:\n                break\n\n    if exact_matches:\n        print(f\"Found {len(exact_matches)} matches for question: '{question}'\")\n        return \"\\n\".join(exact_matches)\n\n    # Step 2: If no exact matches and vectorizer is available, use TF-IDF\n    if vectorizer is not None and tfidf_matrix is not None:\n        try:\n            query_tfidf = vectorizer.transform([question])\n            cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n            \n            # Get the indices of the top_k most similar translations\n            most_similar_indices = cosine_similarities.argsort()[-top_k:][::-1]\n            \n            results = []\n            for idx in most_similar_indices:\n                row = dataset.iloc[idx]\n                similarity_score = cosine_similarities[idx]\n                # Only include results with some minimal similarity\n                if similarity_score > 0.01:  # Lower threshold for Urdu\n                    results.append(f\"Surah {row['Surah']}, Ayah {row['Ayah']}: {row['Translation']} (similarity: {similarity_score:.2f})\")\n            \n            if results:\n                print(f\"Found {len(results)} similar verses using TF-IDF\")\n                return \"\\n\".join(results)\n        except Exception as e:\n            print(f\"Error during TF-IDF similarity: {str(e)}\")\n    \n    # Step 3: Last resort - return a few random verses if no matches\n    import random\n    if len(dataset) > 0:\n        random_indices = random.sample(range(min(len(dataset), 20)), min(top_k, len(dataset)))\n        results = []\n        for idx in random_indices:\n            row = dataset.iloc[idx]\n            results.append(f\"Surah {row['Surah']}, Ayah {row['Ayah']}: {row['Translation']} (random selection)\")\n        \n        print(f\"No matches found. Returning {len(results)} random verses.\")\n        return \"\\n\".join(results)\n    \n    return \"Ù…Ø¹Ø°Ø±ØªØŒ Ú©ÙˆØ¦ÛŒ Ù…ØªØ¹Ù„Ù‚Û Ø¢ÛŒØª Ù†ÛÛŒÚº Ù…Ù„ Ø³Ú©ÛŒÛ”\"\n\ndef urdu_chatbot():\n    \"\"\"Interactive Urdu Quran chatbot function\"\"\"\n    global tokenizer, model\n    \n    print(\"Quranic Urdu Chatbot: Ù‚Ø±Ø¢Ù† Ø³Û’ Ù…ØªØ¹Ù„Ù‚ Ø³ÙˆØ§Ù„ Ù¾ÙˆÚ†Ú¾ÛŒÚºÛ” (exit Ù„Ú©Ú¾ Ú©Ø± Ø¨Ø§ÛØ± Ù†Ú©Ù„ÛŒÚº)\")\n    \n    # Check if we can create the QA pipeline\n    try:\n        qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n        pipeline_available = True\n    except Exception as e:\n        print(f\"Warning: Could not initialize QA pipeline: {str(e)}\")\n        print(\"Will proceed with basic context retrieval only.\")\n        pipeline_available = False\n\n    while True:\n        user_question = input(\"Ø¢Ù¾: \")\n        if user_question.lower() == \"exit\":\n            print(\"Ú†ÙÛŒÙ¹ Ø¨ÙˆÙ¹: Ø§Ù„Ù„Û Ø­Ø§ÙØ¸!\")\n            break\n\n        # Find context\n        try:\n            context = find_relevant_context(user_question, urdu_data)\n            if not context:\n                print(\"Ú†ÙÛŒÙ¹ Ø¨ÙˆÙ¹: Ù…Ø¹Ø°Ø±ØªØŒ Ù…ÛŒÚº Ø§Ø³ Ø³ÙˆØ§Ù„ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ù†ÛÛŒÚº Ø¯Û’ Ø³Ú©ØªØ§Û”\")\n                continue\n            \n            # If we have a pipeline, use it for QA\n            if pipeline_available:\n                try:\n                    result = qa_pipeline(question=user_question, context=context, max_length=512)\n                    print(f\"Ú†ÙÛŒÙ¹ Ø¨ÙˆÙ¹: {result['answer']}\")\n                except Exception as e:\n                    print(f\"Ú†ÙÛŒÙ¹ Ø¨ÙˆÙ¹: {context}\\n\\n(Ù…Ø¹Ø°Ø±ØªØŒ QA Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ù…ÛŒÚº Ù…Ø³Ø¦Ù„Û: {str(e)})\")\n            else:\n                # Just return the context if no pipeline\n                print(f\"Ú†ÙÛŒÙ¹ Ø¨ÙˆÙ¹: {context}\")\n        except Exception as e:\n            print(f\"Ú†ÙÛŒÙ¹ Ø¨ÙˆÙ¹: Ù…Ø¹Ø°Ø±ØªØŒ Ù…ÛŒÚº Ø§Ø³ Ø³ÙˆØ§Ù„ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÙ†Û’ Ø³Û’ Ù‚Ø§ØµØ± ÛÙˆÚºÛ” (Error: {str(e)})\")\n\n# Test the function with a sample question\nprint(\"\\n--- Testing Chatbot ---\")\ntest_question = \"Ø±Ø­Ù…Ù†\"\nprint(f\"Testing with question: '{test_question}'\")\n\ntry:\n    test_context = find_relevant_context(test_question, urdu_data)\n    print(f\"Context for '{test_question}':\")\n    print(test_context)\nexcept Exception as e:\n    print(f\"Error during testing: {str(e)}\")\n\n# Uncomment to run the interactive chatbot\n# urdu_chatbot()\n\nprint(\"\\nScript completed successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:04:43.476647Z","iopub.execute_input":"2025-04-01T18:04:43.476972Z","iopub.status.idle":"2025-04-01T18:04:55.067605Z","shell.execute_reply.started":"2025-04-01T18:04:43.476950Z","shell.execute_reply":"2025-04-01T18:04:55.066626Z"}},"outputs":[{"name":"stdout","text":"Error reading /kaggle/input/urdu-quran-dataset/Urdu.csv: [Errno 2] No such file or directory: '/kaggle/input/urdu-quran-dataset/Urdu.csv'\nError reading /kaggle/input/urdu-quran/Urdu.csv: [Errno 2] No such file or directory: '/kaggle/input/urdu-quran/Urdu.csv'\nSuccessfully read 6237 lines from /kaggle/input/urdu-csv/Urdu.csv\nCreated DataFrame with 6237 verses\n\nProcessed data sample:\n   Surah  Ayah                                        Translation\n0      1     1                            ï»¿Surah,Ayah,Translation\n1      1     2  1,1,Ø´Ø±ÙˆØ¹ Ø§Ù„Ù„Ù‡ Ú©Ø§ Ù†Ø§Ù… Ù„Û’ Ú©Ø± Ø¬Ùˆ Ø¨Ú‘Ø§ Ù…ÛØ±Ø¨Ø§Ù† Ù†ÛØ§ÛŒØª...\n2      1     3  1,2,Ø³Ø¨ Ø·Ø±Ø­ Ú©ÛŒ ØªØ¹Ø±ÛŒÙ Ø®Ø¯Ø§ ÛÛŒ Ú©Ùˆ (Ø³Ø²Ø§ÙˆØ§Ø±) ÛÛ’ Ø¬Ùˆ Øª...\n3      1     4                      1,3,Ø¨Ú‘Ø§ Ù…ÛØ±Ø¨Ø§Ù† Ù†ÛØ§ÛŒØª Ø±Ø­Ù… ÙˆØ§Ù„Ø§\n4      1     5                            1,4,Ø§Ù†ØµØ§Ù Ú©Û’ Ø¯Ù† Ú©Ø§ Ø­Ø§Ú©Ù…\nTotal verses: 6237\n\nInitializing NLP components...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5fe20b81c584ee89857796a23c49b16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf2e931390fc4d9e942b2c7a96c5a4da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56fb44b3d9e24bd6af4e3671258634cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca874e7dcd144be0aede0a4cb5caf86a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc2fc32d12a947d795b3ba6cf421ed16"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Fitting TF-IDF vectorizer...\nTF-IDF vectorizer fitted successfully.\n\n--- Testing Chatbot ---\nTesting with question: 'Ø±Ø­Ù…Ù†'\nNo matches found. Returning 3 random verses.\nContext for 'Ø±Ø­Ù…Ù†':\nSurah 1, Ayah 8: 1,7,Ø§Ù† Ù„ÙˆÚ¯ÙˆÚº Ú©Û’ Ø±Ø³ØªÛ’ Ø¬Ù† Ù¾Ø± ØªÙˆ Ø§Ù¾Ù†Ø§ ÙØ¶Ù„ ÙˆÚ©Ø±Ù… Ú©Ø±ØªØ§ Ø±ÛØ§ Ù†Û Ø§Ù† Ú©Û’ Ø¬Ù† Ù¾Ø± ØºØµÛ’ ÛÙˆØªØ§ Ø±ÛØ§ Ø§ÙˆØ± Ù†Û Ú¯Ù…Ø±Ø§ÛÙˆÚº Ú©Û’ (random selection)\nSurah 1, Ayah 6: 1,5,(Ø§Û’ Ù¾Ø±ÙˆØ±Ø¯Ú¯Ø§Ø±) ÛÙ… ØªÛŒØ±ÛŒ ÛÛŒ Ø¹Ø¨Ø§Ø¯Øª Ú©Ø±ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± ØªØ¬Ú¾ ÛÛŒ Ø³Û’ Ù…Ø¯Ø¯ Ù…Ø§Ù†Ú¯ØªÛ’ ÛÛŒÚº (random selection)\nSurah 2, Ayah 3: 2,5,ÛŒÛÛŒ Ù„ÙˆÚ¯ Ø§Ù¾Ù†Û’ Ù¾Ø±ÙˆØ±Ø¯Ú¯Ø§Ø± (Ú©ÛŒ Ø·Ø±Ù) Ø³Û’ ÛØ¯Ø§ÛŒØª Ù¾Ø± ÛÛŒÚº Ø§ÙˆØ± ÛŒÛÛŒ Ù†Ø¬Ø§Øª Ù¾Ø§Ù†Û’ ÙˆØ§Ù„Û’ ÛÛŒÚº (random selection)\n\nScript completed successfully.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport os\nimport json\nimport pickle\nfrom pathlib import Path\nimport difflib\nfrom collections import defaultdict\nimport logging\nimport warnings\nimport unicodedata\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[logging.StreamHandler()]\n)\nlogger = logging.getLogger('QuranSearchEngine')\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nclass EnhancedQuranSearchEngine:\n    \"\"\"\n    Enhanced Quran Search Engine with multi-technique search capabilities\n    and comprehensive reference handling.\n    \"\"\"\n    \n    def __init__(self, cache_dir=\"/kaggle/working/quran_cache\"):\n        \"\"\"Initialize the search engine with various search techniques\"\"\"\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True, parents=True)\n        \n        self.data = None\n        self.surah_names = {}\n        self.vectorizer = None\n        self.tfidf_matrix = None\n        self.sentence_transformer = None\n        self.sentence_embeddings = None\n        self.qa_model = None\n        self.qa_tokenizer = None\n        self.index_map = {}  # For mapping between different verse numbering systems\n        \n        logger.info(\"Initializing Enhanced Quran Search Engine\")\n        \n    def normalize_arabic_text(self, text):\n        \"\"\"Normalize Arabic/Urdu text by removing diacritics and standardizing characters\"\"\"\n        # Handle None or non-string inputs\n        if not isinstance(text, str):\n            return \"\"\n            \n        # Remove diacritics (harakat)\n        text = ''.join(c for c in unicodedata.normalize('NFKD', text) \n                      if not unicodedata.combining(c))\n        \n        # Standardize certain characters\n        text = text.replace('ÛŒ', 'ÙŠ')  # Standardize ya\n        text = text.replace('Ûƒ', 'Ø©')  # Standardize ta marbutah\n        text = text.replace('Ùƒ', 'Ú©')  # Standardize kaf\n        \n        return text\n    \n    def load_data_from_multiple_sources(self):\n        \"\"\"Load Quran data from multiple possible sources, with fallbacks\"\"\"\n        # 1. Try to load from cache first\n        cache_path = self.cache_dir / \"processed_quran.pkl\"\n        if cache_path.exists():\n            try:\n                logger.info(f\"Loading data from cache: {cache_path}\")\n                with open(cache_path, 'rb') as f:\n                    self.data = pickle.load(f)\n                return True\n            except Exception as e:\n                logger.warning(f\"Failed to load from cache: {e}\")\n        \n        # 2. Try multiple possible file locations\n        possible_paths = [\n            # Standard Kaggle paths\n            '/kaggle/input/urdu-quran-dataset/Urdu.csv',\n            '/kaggle/input/urdu-quran/Urdu.csv',\n            '/kaggle/input/urdu-csv/Urdu.csv',\n            '/kaggle/input/quran-urdu-translation/quran-urdu.csv',\n            '/kaggle/input/urdu-quran-translation/Urdu.csv',\n            '/kaggle/working/Urdu.csv',\n            # Common local paths\n            'data/quran-urdu.csv',\n            'quran-data/Urdu.csv',\n            # Plain text file possibilities\n            '/kaggle/input/urdu-quran-dataset/quran.txt',\n            '/kaggle/input/urdu-quran/quran.txt'\n        ]\n        \n        for path in possible_paths:\n            try:\n                # Try to determine the file type and read appropriately\n                if path.endswith('.csv'):\n                    logger.info(f\"Attempting to read CSV file: {path}\")\n                    # First try with standard CSV format\n                    try:\n                        df = pd.read_csv(path)\n                        if all(col in df.columns for col in ['Surah', 'Ayah', 'Translation']):\n                            self.data = df\n                            logger.info(f\"Successfully loaded standard CSV: {path}\")\n                            break\n                    except Exception as e:\n                        logger.debug(f\"Failed to read as standard CSV: {e}\")\n                    \n                    # Try with delimiters\n                    for delimiter in [',', '|', '\\t']:\n                        try:\n                            df = pd.read_csv(path, delimiter=delimiter, header=None)\n                            if df.shape[1] >= 3:\n                                # Assume first 3 columns are Surah, Ayah, Translation\n                                df.columns = ['Surah', 'Ayah', 'Translation'] + [f'Extra{i}' for i in range(df.shape[1]-3)]\n                                self.data = df\n                                logger.info(f\"Successfully loaded delimited file: {path} with delimiter: {delimiter}\")\n                                break\n                        except Exception as e:\n                            logger.debug(f\"Failed to read with delimiter {delimiter}: {e}\")\n                    \n                    # Try as plain text with no header\n                    if self.data is None:\n                        try:\n                            df = pd.read_csv(path, header=None, names=['Text'], encoding='utf-8')\n                            verses = []\n                            for i, row in df.iterrows():\n                                if not isinstance(row['Text'], str):\n                                    continue\n                                parts = row['Text'].split('|')\n                                if len(parts) >= 3:\n                                    try:\n                                        surah, ayah = int(parts[0]), int(parts[1])\n                                        translation = parts[2]\n                                        verses.append({\"Surah\": surah, \"Ayah\": ayah, \"Translation\": translation})\n                                    except (ValueError, TypeError):\n                                        continue\n                            \n                            if verses:\n                                self.data = pd.DataFrame(verses)\n                                logger.info(f\"Successfully parsed plain text CSV: {path}\")\n                                break\n                        except Exception as e:\n                            logger.debug(f\"Failed to read as plain text: {e}\")\n                \n                # Try as plain text file\n                elif path.endswith('.txt'):\n                    logger.info(f\"Attempting to read text file: {path}\")\n                    try:\n                        with open(path, 'r', encoding='utf-8') as f:\n                            lines = f.readlines()\n                        \n                        verses = []\n                        for i, line in enumerate(lines):\n                            line = line.strip()\n                            if not line:\n                                continue\n                                \n                            # Check if line has a format like \"1:1 - Translation\"\n                            match = re.match(r'(\\d+):(\\d+)\\s*[-â€“]\\s*(.*)', line)\n                            if match:\n                                surah, ayah, translation = match.groups()\n                                verses.append({\n                                    \"Surah\": int(surah),\n                                    \"Ayah\": int(ayah),\n                                    \"Translation\": translation.strip()\n                                })\n                            else:\n                                # Assign artificial numbers\n                                verses.append({\n                                    \"Surah\": (i // 10) + 1, \n                                    \"Ayah\": (i % 10) + 1,\n                                    \"Translation\": line\n                                })\n                        \n                        if verses:\n                            self.data = pd.DataFrame(verses)\n                            logger.info(f\"Successfully parsed text file: {path}\")\n                            break\n                    except Exception as e:\n                        logger.warning(f\"Failed to read text file {path}: {e}\")\n            \n            except Exception as e:\n                logger.warning(f\"Error processing {path}: {e}\")\n                continue\n        \n        # 3. Try to extract from error messages if needed\n        if self.data is None:\n            logger.info(\"Attempting to extract data from error messages\")\n            try:\n                error_files = [\n                    '/kaggle/working/error_log.txt',\n                    'error_log.txt'\n                ]\n                \n                for error_file in error_files:\n                    if os.path.exists(error_file):\n                        with open(error_file, 'r', encoding='utf-8') as f:\n                            error_text = f.read()\n                        \n                        pattern = r\"Error processing row: (.*?), Error:\"\n                        matches = re.findall(pattern, error_text)\n                        \n                        if matches:\n                            verses = []\n                            for i, verse in enumerate(matches):\n                                verses.append({\n                                    \"Surah\": (i // 10) + 1,\n                                    \"Ayah\": (i % 10) + 1,\n                                    \"Translation\": verse.strip()\n                                })\n                            \n                            self.data = pd.DataFrame(verses)\n                            logger.info(f\"Created dataset from error log: {error_file}\")\n                            break\n            except Exception as e:\n                logger.warning(f\"Failed to extract from error log: {e}\")\n        \n        # 4. Create dataset from direct text input\n        if self.data is None and 'paste.txt' in possible_paths:\n            logger.info(\"Attempting to create dataset from paste.txt content\")\n            try:\n                with open('paste.txt', 'r', encoding='utf-8') as f:\n                    text = f.read()\n                \n                # Extract error messages containing verses\n                pattern = r\"Error processing row: (.*?), Error: invalid literal for int\\(\\)\"\n                matches = re.findall(pattern, text)\n                \n                if matches:\n                    verses = []\n                    for i, verse in enumerate(matches):\n                        verses.append({\n                            \"Surah\": (i // 10) + 1,\n                            \"Ayah\": (i % 10) + 1,\n                            \"Translation\": verse.strip()\n                        })\n                    \n                    self.data = pd.DataFrame(verses)\n                    logger.info(f\"Created dataset from paste.txt content with {len(verses)} verses\")\n            except Exception as e:\n                logger.warning(f\"Failed to extract from paste.txt: {e}\")\n        \n        # 5. Use hard-coded fallback data if all else fails\n        if self.data is None:\n            logger.warning(\"All data loading methods failed. Using fallback dataset.\")\n            self.data = pd.DataFrame([\n                {\"Surah\": 1, \"Ayah\": 1, \"Translation\": \"Ø¨Ø³Ù… Ø§Ù„Ù„Û Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÛŒÙ…\"},\n                {\"Surah\": 1, \"Ayah\": 2, \"Translation\": \"Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Û Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÛŒÙ†\"},\n                {\"Surah\": 1, \"Ayah\": 3, \"Translation\": \"Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÛŒÙ…\"},\n                {\"Surah\": 1, \"Ayah\": 4, \"Translation\": \"Ù…Ø§Ù„Ú© ÛŒÙˆÙ… Ø§Ù„Ø¯ÛŒÙ†\"},\n                {\"Surah\": 1, \"Ayah\": 5, \"Translation\": \"Ø§ÛŒØ§Ú© Ù†Ø¹Ø¨Ø¯ Ùˆ Ø§ÛŒØ§Ú© Ù†Ø³ØªØ¹ÛŒÙ†\"},\n                {\"Surah\": 1, \"Ayah\": 6, \"Translation\": \"Ø§Ú¾Ø¯Ù†Ø§ Ø§Ù„ØµØ±Ø§Ø· Ø§Ù„Ù…Ø³ØªÙ‚ÛŒÙ…\"},\n                {\"Surah\": 1, \"Ayah\": 7, \"Translation\": \"ØµØ±Ø§Ø· Ø§Ù„Ø°ÛŒÙ† Ø§Ù†Ø¹Ù…Øª Ø¹Ù„ÛŒÚ¾Ù… ØºÛŒØ± Ø§Ù„Ù…ØºØ¶ÙˆØ¨ Ø¹Ù„ÛŒÚ¾Ù… Ùˆ Ù„Ø§ Ø§Ù„Ø¶Ø§Ù„ÛŒÙ†\"},\n                {\"Surah\": 2, \"Ayah\": 1, \"Translation\": \"Ø§Ù„Ù…\"},\n                {\"Surah\": 2, \"Ayah\": 2, \"Translation\": \"Ø°Ù°Ù„ÙÚ©Ù Ø§Ù„Ù’Ú©ÙØªÙ°Ø¨Ù Ù„ÙØ§ Ø±ÙÛŒÙ’Ø¨Ù Ûš ÙÙÛŒÙ’ÛÙ Ûš ÛÙØ¯Ù‹ÛŒ Ù„ÙÙ‘Ù„Ù’Ù…ÙØªÙÙ‘Ù‚ÙÛŒÙ’Ù†Ù\"},\n                {\"Surah\": 112, \"Ayah\": 1, \"Translation\": \"Ù‚Ù„ Ù‡Ùˆ Ø§Ù„Ù„Ù‡ Ø§Ø­Ø¯\"},\n                {\"Surah\": 112, \"Ayah\": 2, \"Translation\": \"Ø§Ù„Ù„Ù‡ Ø§Ù„ØµÙ…Ø¯\"},\n                {\"Surah\": 112, \"Ayah\": 3, \"Translation\": \"Ù„Ù… ÙŠÙ„Ø¯ ÙˆÙ„Ù… ÙŠÙˆÙ„Ø¯\"},\n                {\"Surah\": 112, \"Ayah\": 4, \"Translation\": \"ÙˆÙ„Ù… ÙŠÙƒÙ† Ù„Ù‡ ÙƒÙÙˆØ§ Ø§Ø­Ø¯\"}\n            ])\n        \n        # Load surah names\n        self.load_surah_names()\n        \n        # Cache the processed data\n        try:\n            with open(cache_path, 'wb') as f:\n                pickle.dump(self.data, f)\n            logger.info(f\"Cached processed data to {cache_path}\")\n        except Exception as e:\n            logger.warning(f\"Failed to cache data: {e}\")\n        \n        # Add normalized text column for improved matching\n        if 'Translation' in self.data.columns:\n            self.data['NormalizedText'] = self.data['Translation'].apply(self.normalize_arabic_text)\n            \n            # Add combined reference string for convenience\n            self.data['Reference'] = self.data.apply(\n                lambda x: f\"Surah {x['Surah']}:{x['Ayah']}\" + \n                        (f\" ({self.surah_names.get(x['Surah'], '')})\" if x['Surah'] in self.surah_names else \"\"), \n                axis=1\n            )\n        else:\n            logger.error(\"Data loaded but 'Translation' column not found. Check data format.\")\n            return False\n        \n        return True\n    \n    def load_surah_names(self):\n        \"\"\"Load Surah names from various possible sources\"\"\"\n        # First try to load from cache\n        cache_path = self.cache_dir / \"surah_names.json\"\n        if cache_path.exists():\n            try:\n                with open(cache_path, 'r', encoding='utf-8') as f:\n                    self.surah_names = json.load(f)\n                return\n            except:\n                pass\n        \n        # Hardcoded names as fallback\n        self.surah_names = {\n            1: \"Ø§Ù„ÙØ§ØªØ­Ø© (Al-Fatiha)\",\n            2: \"Ø§Ù„Ø¨Ù‚Ø±Ø© (Al-Baqara)\",\n            3: \"Ø¢Ù„ Ø¹Ù…Ø±Ø§Ù† (Aal-Imran)\",\n            4: \"Ø§Ù„Ù†Ø³Ø§Ø¡ (An-Nisa)\",\n            5: \"Ø§Ù„Ù…Ø§Ø¦Ø¯Ø© (Al-Ma'ida)\",\n            6: \"Ø§Ù„Ø£Ù†Ø¹Ø§Ù… (Al-An'am)\",\n            112: \"Ø§Ù„Ø¥Ø®Ù„Ø§Øµ (Al-Ikhlas)\",\n            113: \"Ø§Ù„ÙÙ„Ù‚ (Al-Falaq)\",\n            114: \"Ø§Ù„Ù†Ø§Ø³ (An-Nas)\"\n        }\n        \n        # Try to find a surah names file\n        possible_paths = [\n            '/kaggle/input/quran-metadata/surah_names.json',\n            '/kaggle/input/quran-urdu-translation/surah_names.json',\n            'data/surah_names.json'\n        ]\n        \n        for path in possible_paths:\n            try:\n                with open(path, 'r', encoding='utf-8') as f:\n                    self.surah_names = json.load(f)\n                logger.info(f\"Loaded surah names from {path}\")\n                break\n            except:\n                continue\n        \n        # Cache the names\n        try:\n            with open(cache_path, 'w', encoding='utf-8') as f:\n                json.dump(self.surah_names, f, ensure_ascii=False, indent=2)\n        except:\n            pass\n    \n    def initialize_search_methods(self):\n        \"\"\"Initialize multiple search methods for robust text retrieval\"\"\"\n        if self.data is None or len(self.data) == 0:\n            logger.error(\"No data available. Please load data first.\")\n            return False\n        \n        # 1. Initialize TF-IDF vectorizer\n        logger.info(\"Initializing TF-IDF vectorizer...\")\n        try:\n            self.vectorizer = TfidfVectorizer(\n                min_df=1, \n                max_df=0.9,\n                ngram_range=(1, 3),\n                sublinear_tf=True\n            )\n            self.tfidf_matrix = self.vectorizer.fit_transform(self.data['NormalizedText'])\n            logger.info(f\"TF-IDF matrix shape: {self.tfidf_matrix.shape}\")\n        except Exception as e:\n            logger.warning(f\"Failed to initialize TF-IDF: {e}\")\n        \n        # 2. Try to initialize sentence transformer (if available)\n        try:\n            from sentence_transformers import SentenceTransformer\n            \n            # Try loading cached embeddings first\n            embeddings_path = self.cache_dir / \"sentence_embeddings.npy\"\n            if embeddings_path.exists():\n                try:\n                    self.sentence_embeddings = np.load(embeddings_path)\n                    logger.info(f\"Loaded sentence embeddings from cache: {embeddings_path}\")\n                    self.sentence_transformer = True  # Just a flag that we have embeddings\n                except Exception as e:\n                    logger.warning(f\"Failed to load cached embeddings: {e}\")\n            \n            # If not loaded from cache, try to generate them\n            if self.sentence_embeddings is None:\n                try:\n                    # Try multilingual model first\n                    model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n                    self.sentence_transformer = SentenceTransformer(model_name)\n                    \n                    # Compute embeddings (may take time for large datasets)\n                    logger.info(f\"Computing sentence embeddings using {model_name}...\")\n                    self.sentence_embeddings = self.sentence_transformer.encode(\n                        self.data['Translation'].tolist(), \n                        show_progress_bar=True,\n                        batch_size=32\n                    )\n                    \n                    # Cache the embeddings\n                    try:\n                        np.save(embeddings_path, self.sentence_embeddings)\n                        logger.info(f\"Cached sentence embeddings to {embeddings_path}\")\n                    except Exception as e:\n                        logger.warning(f\"Failed to cache embeddings: {e}\")\n                        \n                except Exception as e:\n                    logger.warning(f\"Failed to initialize sentence transformer: {e}\")\n                    self.sentence_transformer = None\n        except ImportError:\n            logger.info(\"SentenceTransformer not available. Skipping semantic search capabilities.\")\n            self.sentence_transformer = None\n        \n        # 3. Initialize QA model if transformers is available\n        try:\n            from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n            \n            # Only initialize if data is large enough to be useful\n            if len(self.data) > 10:\n                logger.info(\"Initializing QA model...\")\n                try:\n                    model_name = \"bert-base-multilingual-cased\"\n                    self.qa_tokenizer = AutoTokenizer.from_pretrained(model_name)\n                    self.qa_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n                    logger.info(\"QA model initialized successfully\")\n                except Exception as e:\n                    logger.warning(f\"Failed to initialize QA model: {e}\")\n        except ImportError:\n            logger.info(\"Transformers not available. Skipping QA capabilities.\")\n        \n        logger.info(\"Search methods initialization complete\")\n        return True\n    \n    def search(self, query, top_k=5, include_partial=True, include_similar=True, threshold=0.1):\n        \"\"\"\n        Search for verses matching the query using multiple techniques\n        \n        Args:\n            query: The search query text\n            top_k: Number of results to return\n            include_partial: Whether to include partial matches\n            include_similar: Whether to include semantically similar results\n            threshold: Similarity threshold for including results\n            \n        Returns:\n            Dictionary with primary match and other matches\n        \"\"\"\n        if self.data is None or len(self.data) == 0:\n            return {\"error\": \"No data available. Please load data first.\"}\n        \n        # Handle empty query\n        if not query or not isinstance(query, str):\n            return {\"error\": \"Empty or invalid query\", \"primary_match\": None, \"other_matches\": [], \"total_matches\": 0}\n        \n        # Normalize the query\n        normalized_query = self.normalize_arabic_text(query)\n        \n        # Dictionary to store all matches with their scores and methods\n        all_matches = defaultdict(lambda: {\"score\": 0, \"methods\": []})\n        \n        # 1. Look for exact matches first (highest priority)\n        exact_indices = []\n        for idx, row in self.data.iterrows():\n            if query in row['Translation'] or normalized_query in row['NormalizedText']:\n                match_key = f\"{row['Surah']}:{row['Ayah']}\"\n                all_matches[match_key][\"verse\"] = row['Translation']\n                all_matches[match_key][\"reference\"] = row['Reference']\n                all_matches[match_key][\"score\"] += 10  # High score for exact match\n                all_matches[match_key][\"methods\"].append(\"exact\")\n                exact_indices.append(idx)\n        \n        # 2. Use TF-IDF for partial keyword matching\n        if self.vectorizer is not None and (include_partial or len(all_matches) < top_k):\n            try:\n                query_vec = self.vectorizer.transform([normalized_query])\n                similarity_scores = cosine_similarity(query_vec, self.tfidf_matrix).flatten()\n                \n                # Get indices of top matches\n                top_indices = similarity_scores.argsort()[-top_k*2:][::-1]\n                \n                for idx in top_indices:\n                    if similarity_scores[idx] > threshold:\n                        row = self.data.iloc[idx]\n                        match_key = f\"{row['Surah']}:{row['Ayah']}\"\n                        \n                        # Only add if not already an exact match or update score if better\n                        if match_key not in all_matches or all_matches[match_key][\"score\"] < similarity_scores[idx] * 5:\n                            all_matches[match_key][\"verse\"] = row['Translation']\n                            all_matches[match_key][\"reference\"] = row['Reference']\n                            all_matches[match_key][\"score\"] = max(all_matches[match_key][\"score\"], similarity_scores[idx] * 5)\n                            all_matches[match_key][\"methods\"].append(\"tfidf\")\n            except Exception as e:\n                logger.warning(f\"TF-IDF search failed: {e}\")\n        \n        # 3. Use semantic search with sentence embeddings\n        if self.sentence_transformer is not None and self.sentence_embeddings is not None and include_similar:\n            try:\n                # If it's just a flag, we only have cached embeddings\n                if isinstance(self.sentence_transformer, bool):\n                    # Use a simpler approach with dot product\n                    from sentence_transformers import SentenceTransformer\n                    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n                    query_embedding = model.encode([query])[0]\n                    \n                    # Calculate similarities\n                    similarities = np.dot(self.sentence_embeddings, query_embedding)\n                    top_indices = similarities.argsort()[-top_k*2:][::-1]\n                    \n                    for idx in top_indices:\n                        if similarities[idx] > threshold:\n                            row = self.data.iloc[idx]\n                            match_key = f\"{row['Surah']}:{row['Ayah']}\"\n                            \n                            all_matches[match_key][\"verse\"] = row['Translation']\n                            all_matches[match_key][\"reference\"] = row['Reference']\n                            all_matches[match_key][\"score\"] = max(all_matches[match_key][\"score\"], similarities[idx] * 3)\n                            all_matches[match_key][\"methods\"].append(\"semantic\")\n                else:\n                    # Use the model directly\n                    query_embedding = self.sentence_transformer.encode([query])[0]\n                    \n                    # Calculate similarities\n                    similarities = np.dot(self.sentence_embeddings, query_embedding)\n                    top_indices = similarities.argsort()[-top_k*2:][::-1]\n                    \n                    for idx in top_indices:\n                        if similarities[idx] > threshold:\n                            row = self.data.iloc[idx]\n                            match_key = f\"{row['Surah']}:{row['Ayah']}\"\n                            \n                            all_matches[match_key][\"verse\"] = row['Translation']\n                            all_matches[match_key][\"reference\"] = row['Reference']\n                            all_matches[match_key][\"score\"] = max(all_matches[match_key][\"score\"], similarities[idx] * 3)\n                            all_matches[match_key][\"methods\"].append(\"semantic\")\n            except Exception as e:\n                logger.warning(f\"Semantic search failed: {e}\")\n        \n        # 4. Use fuzzy matching for query with typos\n        if include_partial and len(all_matches) < top_k:\n            for idx, row in self.data.iterrows():\n                # Skip already matched verses\n                match_key = f\"{row['Surah']}:{row['Ayah']}\"\n                if match_key in all_matches:\n                    continue\n                \n                # Calculate fuzzy match ratio\n                ratio = difflib.SequenceMatcher(None, normalized_query, row['NormalizedText']).ratio()\n                \n                if ratio > max(0.6, threshold * 2):  # Higher threshold for fuzzy matching\n                    all_matches[match_key][\"verse\"] = row['Translation']\n                    all_matches[match_key][\"reference\"] = row['Reference']\n                    all_matches[match_key][\"score\"] = max(all_matches[match_key][\"score\"], ratio * 2)\n                    all_matches[match_key][\"methods\"].append(\"fuzzy\")\n        \n        # Sort matches by score\n        sorted_matches = sorted(all_matches.items(), key=lambda x: x[1][\"score\"], reverse=True)\n        \n        # Prepare results\n        results = {\n            \"primary_match\": None,\n            \"other_matches\": [],\n            \"total_matches\": len(sorted_matches)\n        }\n        \n        # Set primary match (the one with highest score)\n        if sorted_matches:\n            primary = sorted_matches[0][1]\n            results[\"primary_match\"] = {\n                \"verse\": primary[\"verse\"],\n                \"reference\": primary[\"reference\"],\n                \"score\": primary[\"score\"],\n                \"methods\": primary[\"methods\"]\n            }\n            \n            # Add other matches\n            other_matches = []\n            for _, match in sorted_matches[1:top_k]:\n                other_matches.append({\n                    \"verse\": match[\"verse\"],\n                    \"reference\": match[\"reference\"],\n                    \"score\": match[\"score\"],\n                    \"methods\": match[\"methods\"]\n                })\n            \n            results[\"other_matches\"] = other_matches\n        \n        return results\n    \n    def answer_question(self, question, context=None, max_length=512):\n        \"\"\"\n        Answer a question using the QA model.\n        \n        Args:\n            question: The question to answer\n            context: Optional context to use, otherwise will search for relevant verses\n            \n        Returns:\n            Dictionary with answer and reference\n        \"\"\"\n        if self.qa_model is None or self.qa_tokenizer is None:\n            return {\"error\": \"QA model not available\"}\n        \n        try:\n            from transformers import pipeline\n            qa_pipeline = pipeline(\"question-answering\", model=self.qa_model, tokenizer=self.qa_tokenizer)\n            \n            # Get context if not provided\n            if context is None:\n                search_results = self.search(question, top_k=3)\n                \n                if search_results[\"primary_match\"]:\n                    context = search_results[\"primary_match\"][\"verse\"]\n                    \n                    # Add some additional context if available\n                    for match in search_results[\"other_matches\"][:2]:\n                        context += \" \" + match[\"verse\"]\n                else:\n                    return {\"error\": \"No relevant context found for the question\"}\n            \n            # Use the QA pipeline to get an answer\n            result = qa_pipeline(question=question, context=context, max_length=max_length)\n            \n            return {\n                \"answer\": result[\"answer\"],\n                \"score\": result[\"score\"],\n                \"context\": context\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error in answer_question: {e}\")\n            return {\"error\": f\"Failed to answer question: {str(e)}\"}\n\n    def formatted_search_results(self, query, top_k=5):\n        \"\"\"\n        Return search results in a nicely formatted string.\n        \n        Args:\n            query: The search query\n            top_k: Number of results to return\n            \n        Returns:\n            Formatted string with search results\n        \"\"\"\n        results = self.search(query, top_k=top_k)\n        \n        if \"error\" in results:\n            return f\"Error: {results['error']}\"\n        \n        output = [f\"Search results for: '{query}'\"]\n        output.append(\"=\" * 50)\n        \n        if results[\"primary_match\"]:\n            primary = results[\"primary_match\"]\n            output.append(\"Primary Match:\")\n            output.append(f\"ğŸ“– {primary['reference']}\")\n            output.append(f\"ğŸ“ {primary['verse']}\")\n            output.append(f\"âœ“ Match score: {primary['score']:.2f} using {', '.join(primary['methods'])}\")\n            output.append(\"-\" * 50)\n        else:\n            output.append(\"No primary match found.\")\n            output.append(\"-\" * 50)\n        \n        if results[\"other_matches\"]:\n            output.append(\"Other Relevant Matches:\")\n            for i, match in enumerate(results[\"other_matches\"], 1):\n                output.append(f\"{i}. ğŸ“– {match['reference']}\")\n                output.append(f\"   ğŸ“ {match['verse']}\")\n                output.append(f\"   âœ“ Match score: {match['score']:.2f} using {', '.join(match['methods'])}\")\n                output.append(\"   \" + \"-\" * 40)\n        else:\n            output.append(\"No other matches found.\")\n        \n        output.append(f\"\\nTotal matches: {results['total_matches']}\")\n        \n        return \"\\n\".join(output)\n\ndef initialize_search_engine():\n    \"\"\"Helper function to initialize the search engine\"\"\"\n    engine = EnhancedQuranSearchEngine()\n    success = engine.load_data_from_multiple_sources()\n    \n    if success:\n        engine.initialize_search_methods()\n        logger.info(f\"Engine initialized with {len(engine.data)} verses\")\n        return engine\n    else:\n        logger.error(\"Failed to initialize search engine\")\n        return None\n\ndef main():\n    \"\"\"Main function to demonstrate search capabilities\"\"\"\n    logger.info(\"Starting Quran Search Engine\")\n    \n    # Initialize engine\n    engine = initialize_search_engine()\n    \n    if not engine:\n        logger.error(\"Engine initialization failed. Exiting.\")\n        return\n    \n    # Example search\n    test_queries = [\n        \"Ø±Ø­Ù…Ù†\",\n        \"Ù†Ù…Ø§Ø²\",\n        \"Ø¬Ù†Øª\",\n        \"ØªÙˆØ¨Ù‡\",\n        \"ØµØ¨Ø±\"\n    ]\n    \n    for query in test_queries:\n        logger.info(f\"\\nTesting search for: '{query}'\")\n        results = engine.formatted_search_results(query)\n        print(results)\n    \n    # Interactive mode\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Interactive Quran Search Engine\")\n    print(\"=\" * 60)\n    print(\"Enter your search query (or 'exit' to quit):\")\n    \n    while True:\n        query = input(\"\\nSearch: \")\n        if query.lower() in ['exit', 'quit', 'q']:\n            break\n        \n        if not query.strip():\n            continue\n            \n        results = engine.formatted_search_results(query)\n        print(results)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:14:42.279409Z","iopub.execute_input":"2025-04-01T18:14:42.279763Z","iopub.status.idle":"2025-04-01T18:17:21.228607Z","shell.execute_reply.started":"2025-04-01T18:14:42.279739Z","shell.execute_reply":"2025-04-01T18:17:21.227834Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a02c9356917b4074b13cbd34e8fa9301"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd2ec0d497ef401f9baf9cfd28023aff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a7e79e3d0ba461d8baa87b265b5a04e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a8ea04896645c69cefe1fec7065f24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d27de39dd3614196a203d0820dfe01a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee1217f89d7f452bacca73b52633b2b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4eca20b52b44c98917d660ed6dd9316"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ba43b5557b94353955f059a5021a677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe4bee062774498b9e475a9b9b3cef0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"861b97984c0044ae8d5ba65a8b3069fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/195 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c592635d43754f6c9372c017afec325f"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ea729eff30e4525bd2b1c12617d37b6"}},"metadata":{}},{"name":"stdout","text":"Search results for: 'Ø±Ø­Ù…Ù†'\n==================================================\nPrimary Match:\nğŸ“– Surah 32:1\nğŸ“ Ø§Ù„Ù“Ù…Ù“\nâœ“ Match score: 28.49 using semantic\n--------------------------------------------------\nOther Relevant Matches:\n1. ğŸ“– Surah 31:1\n   ğŸ“ Ø§Ù„Ù“Ù…Ù“\n   âœ“ Match score: 28.49 using semantic\n   ----------------------------------------\n2. ğŸ“– Surah 30:1\n   ğŸ“ Ø§Ù„Ù“Ù…Ù“\n   âœ“ Match score: 28.49 using semantic\n   ----------------------------------------\n3. ğŸ“– Surah 42:1\n   ğŸ“ Ø­Ù°Ù…Ù“\n   âœ“ Match score: 28.00 using semantic\n   ----------------------------------------\n4. ğŸ“– Surah 40:1\n   ğŸ“ Ø­Ù°Ù…\n   âœ“ Match score: 27.60 using semantic\n   ----------------------------------------\n\nTotal matches: 22\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb75935b01f14eda98c704e987569aa3"}},"metadata":{}},{"name":"stdout","text":"Search results for: 'Ù†Ù…Ø§Ø²'\n==================================================\nPrimary Match:\nğŸ“– Surah 70:22\nğŸ“ Ù…Ú¯Ø± Ù†Ù…Ø§Ø² Ú¯Ø²Ø§Ø±\nâœ“ Match score: 57.42 using exact, semantic\n--------------------------------------------------\nOther Relevant Matches:\n1. ğŸ“– Surah 96:10\n   ğŸ“ (ÛŒØ¹Ù†ÛŒ) Ø§ÛŒÚ© Ø¨Ù†Ø¯Û’ Ú©Ùˆ Ø¬Ø¨ ÙˆÛ Ù†Ù…Ø§Ø² Ù¾Ú‘Ú¾Ù†Û’ Ù„Ú¯ØªØ§ ÛÛ’\n   âœ“ Match score: 56.05 using exact, semantic\n   ----------------------------------------\n2. ğŸ“– Surah 108:2\n   ğŸ“ ØªÙˆ Ø§Ù¾Ù†Û’ Ù¾Ø±ÙˆØ±Ø¯Ú¯Ø§Ø± Ú©Û’ Ù„ÛŒÛ’ Ù†Ù…Ø§Ø² Ù¾Ú‘Ú¾Ø§ Ú©Ø±Ùˆ Ø§ÙˆØ± Ù‚Ø±Ø¨Ø§Ù†ÛŒ Ø¯ÛŒØ§ Ú©Ø±Ùˆ\n   âœ“ Match score: 55.77 using exact, semantic\n   ----------------------------------------\n3. ğŸ“– Surah 23:2\n   ğŸ“ Ø¬Ùˆ Ù†Ù…Ø§Ø² Ù…ÛŒÚº Ø¹Ø¬Ø²Ùˆ Ù†ÛŒØ§Ø² Ú©Ø±ØªÛ’ ÛÛŒÚº\n   âœ“ Match score: 53.31 using exact, semantic\n   ----------------------------------------\n4. ğŸ“– Surah 70:34\n   ğŸ“ Ø§ÙˆØ± Ø¬Ùˆ Ø§Ù¾Ù†ÛŒ Ù†Ù…Ø§Ø² Ú©ÛŒ Ø®Ø¨Ø± Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚº\n   âœ“ Match score: 51.55 using exact, semantic\n   ----------------------------------------\n\nTotal matches: 101\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e981fb1c5ab42d693b861076c82a81f"}},"metadata":{}},{"name":"stdout","text":"Search results for: 'Ø¬Ù†Øª'\n==================================================\nPrimary Match:\nğŸ“– Surah 74:3\nğŸ“ Ø§ÙˆØ± Ø§Ù¾Ù†Û’ Ù¾Ø±ÙˆØ±Ø¯Ú¯Ø§Ø± Ú©ÛŒ Ø¨Ú‘Ø§Ø¦ÛŒ Ú©Ø±Ùˆ\nâœ“ Match score: 38.05 using semantic\n--------------------------------------------------\nOther Relevant Matches:\n1. ğŸ“– Surah 53:15\n   ğŸ“ Ø§Ø³ÛŒ Ú©Û’ Ù¾Ø§Ø³ Ø±ÛÙ†Û’ Ú©ÛŒ Ø¬Ù†Øª ÛÛ’\n   âœ“ Match score: 36.42 using exact, semantic\n   ----------------------------------------\n2. ğŸ“– Surah 70:22\n   ğŸ“ Ù…Ú¯Ø± Ù†Ù…Ø§Ø² Ú¯Ø²Ø§Ø±\n   âœ“ Match score: 35.99 using semantic\n   ----------------------------------------\n3. ğŸ“– Surah 43:72\n   ğŸ“ Ø§ÙˆØ± ÛŒÛ Ø¬Ù†Øª Ø¬Ø³ Ú©Û’ ØªÙ… Ù…Ø§Ù„Ú© Ú©Ø± Ø¯ÛŒØ¦Û’ Ú¯Ø¦Û’ ÛÙˆ ØªÙ…ÛØ§Ø±Û’ Ø§Ø¹Ù…Ø§Ù„ Ú©Ø§ ØµÙ„Û ÛÛ’\n   âœ“ Match score: 35.45 using exact, semantic\n   ----------------------------------------\n4. ğŸ“– Surah 36:58\n   ğŸ“ Ù¾Ø±ÙˆØ±Ø¯Ú¯Ø§Ø± Ù…ÛØ±Ø¨Ø§Ù† Ú©ÛŒ Ø·Ø±Ù Ø³Û’ Ø³Ù„Ø§Ù… (Ú©ÛØ§ Ø¬Ø§Ø¦Û’ Ú¯Ø§)\n   âœ“ Match score: 35.27 using semantic\n   ----------------------------------------\n\nTotal matches: 27\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d64340eb394bcb9ccd6de11a053978"}},"metadata":{}},{"name":"stdout","text":"Search results for: 'ØªÙˆØ¨Ù‡'\n==================================================\nPrimary Match:\nğŸ“– Surah 2:52 (Ø§Ù„Ø¨Ù‚Ø±Ø© (Al-Baqara))\nğŸ“ Ù¾Ú¾Ø± Ø§Ø³ Ú©Û’ Ø¨Ø¹Ø¯ ÛÙ… Ù†Û’ ØªÙ… Ú©Ùˆ Ù…Ø¹Ø§Ù Ú©Ø± Ø¯ÛŒØ§ØŒ ØªØ§Ú©Û ØªÙ… Ø´Ú©Ø± Ú©Ø±Ùˆ\nâœ“ Match score: 36.72 using semantic\n--------------------------------------------------\nOther Relevant Matches:\n1. ğŸ“– Surah 42:43\n   ğŸ“ Ø§ÙˆØ± Ø¬Ùˆ ØµØ¨Ø± Ú©Ø±Û’ Ø§ÙˆØ± Ù‚ØµÙˆØ± Ù…Ø¹Ø§Ù Ú©Ø±Ø¯Û’ ØªÙˆ ÛŒÛ ÛÙ…Øª Ú©Û’ Ú©Ø§Ù… ÛÛŒÚº\n   âœ“ Match score: 34.46 using semantic\n   ----------------------------------------\n2. ğŸ“– Surah 23:106\n   ğŸ“ Ø§Û’ ÛÙ…Ø§Ø±Û’ Ù¾Ø±ÙˆØ±Ø¯Ú¯Ø§Ø±! ÛÙ… Ù¾Ø± ÛÙ…Ø§Ø±ÛŒ Ú©Ù… Ø¨Ø®ØªÛŒ ØºØ§Ù„Ø¨ ÛÙˆÚ¯Ø¦ÛŒ Ø§ÙˆØ± ÛÙ… Ø±Ø³ØªÛ’ Ø³Û’ Ø¨Ú¾Ù¹Ú© Ú¯Ø¦Û’\n   âœ“ Match score: 31.36 using semantic\n   ----------------------------------------\n3. ğŸ“– Surah 2:160 (Ø§Ù„Ø¨Ù‚Ø±Ø© (Al-Baqara))\n   ğŸ“ ÛØ§Úº Ø¬Ùˆ ØªÙˆØ¨Û Ú©Ø±ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ø§Ù¾Ù†ÛŒ Ø­Ø§Ù„Øª Ø¯Ø±Ø³Øª Ú©Ø±Ù„ÛŒØªÛ’ Ø§ÙˆØ± (Ø§Ø­Ú©Ø§Ù… Ø§Ù„ÛÛŒÙ° Ú©Ùˆ) ØµØ§Ù ØµØ§Ù Ø¨ÛŒØ§Ù† Ú©Ø±Ø¯ÛŒØªÛ’ ÛÛŒÚº ØªÙˆ Ù…ÛŒÚº Ø§Ù† Ú©Û’ Ù‚ØµÙˆØ± Ù…Ø¹Ø§Ù Ú©Ø±Ø¯ÛŒØªØ§ ÛÙˆÚº Ø§ÙˆØ± Ù…ÛŒÚº Ø¨Ú‘Ø§ Ù…Ø¹Ø§Ù Ú©Ø±Ù†Û’ ÙˆØ§Ù„Ø§ (Ø§ÙˆØ±) Ø±Ø­Ù… ÙˆØ§Ù„Ø§ ÛÙˆÚº\n   âœ“ Match score: 31.21 using semantic\n   ----------------------------------------\n4. ğŸ“– Surah 75:35\n   ğŸ“ Ù¾Ú¾Ø± Ø§ÙØ³ÙˆØ³ ÛÛ’ ØªØ¬Ú¾ Ù¾Ø± Ù¾Ú¾Ø± Ø§ÙØ³ÙˆØ³ ÛÛ’\n   âœ“ Match score: 30.97 using semantic\n   ----------------------------------------\n\nTotal matches: 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976a8fcc45ac492bbfb666882666a37f"}},"metadata":{}},{"name":"stdout","text":"Search results for: 'ØµØ¨Ø±'\n==================================================\nPrimary Match:\nğŸ“– Surah 44:59\nğŸ“ Ù¾Ø³ ØªÙ… Ø¨Ú¾ÛŒ Ø§Ù†ØªØ¸Ø§Ø± Ú©Ø±Ùˆ ÛŒÛ Ø¨Ú¾ÛŒ Ø§Ù†ØªØ¸Ø§Ø± Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº\nâœ“ Match score: 39.91 using semantic\n--------------------------------------------------\nOther Relevant Matches:\n1. ğŸ“– Surah 74:7\n   ğŸ“ Ø§ÙˆØ± Ø§Ù¾Ù†Û’ Ù¾Ø±ÙˆØ±Ø¯Ú¯Ø§Ø± Ú©Û’ Ù„Ø¦Û’ ØµØ¨Ø± Ú©Ø±Ùˆ\n   âœ“ Match score: 38.87 using exact, semantic\n   ----------------------------------------\n2. ğŸ“– Surah 11:122\n   ğŸ“ Ø§ÙˆØ± (Ù†ØªÛŒØ¬ÛÙ´ Ø§Ø¹Ù…Ø§Ù„ Ú©Ø§) ØªÙ… Ø¨Ú¾ÛŒ Ø§Ù†ØªØ¸Ø§Ø± Ú©Ø±ÙˆØŒ ÛÙ… Ø¨Ú¾ÛŒ Ø§Ù†ØªØ¸Ø§Ø± Ú©Ø±ØªÛ’ ÛÛŒÚº\n   âœ“ Match score: 36.30 using semantic\n   ----------------------------------------\n3. ğŸ“– Surah 52:31\n   ğŸ“ Ú©ÛÛ Ø¯Ùˆ Ú©Û Ø§Ù†ØªØ¸Ø§Ø± Ú©Ø¦Û’ Ø¬Ø§Ø¤ Ù…ÛŒÚº Ø¨Ú¾ÛŒ ØªÙ…ÛØ§Ø±Û’ Ø³Ø§ØªÚ¾ Ø§Ù†ØªØ¸Ø§Ø± Ú©Ø±ØªØ§ ÛÙˆÚº\n   âœ“ Match score: 30.51 using semantic\n   ----------------------------------------\n4. ğŸ“– Surah 73:2\n   ğŸ“ Ø±Ø§Øª Ú©Ùˆ Ù‚ÛŒØ§Ù… Ú©ÛŒØ§ Ú©Ø±Ùˆ Ù…Ú¯Ø± ØªÚ¾ÙˆÚ‘ÛŒ Ø³ÛŒ Ø±Ø§Øª\n   âœ“ Match score: 29.43 using semantic\n   ----------------------------------------\n\nTotal matches: 77\n\n============================================================\nInteractive Quran Search Engine\n============================================================\nEnter your search query (or 'exit' to quit):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nSearch:  exit\n"}],"execution_count":3},{"cell_type":"code","source":"# kaggle_create_model.py\n\"\"\"\nThis script is designed to run on Kaggle to:\n1. Create the Quran search engine model\n2. Save it in a format that can be downloaded\n\"\"\"\nimport pickle\nimport os\nimport sys\nfrom pathlib import Path\nimport logging\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger('KaggleModelCreation')\n\n\n\ndef create_and_save_model():\n    \"\"\"Create and save the search engine model on Kaggle\"\"\"\n    # Create output directory for the model\n    output_dir = Path('/kaggle/working/model_output')\n    output_dir.mkdir(exist_ok=True, parents=True)\n    \n    # Initialize the search engine\n    logger.info(\"Initializing search engine...\")\n    engine = EnhancedQuranSearchEngine(cache_dir=str(output_dir))\n    \n    # Load data and initialize search methods\n    success = engine.load_data_from_multiple_sources()\n    if not success:\n        logger.error(\"Failed to load data for search engine\")\n        return False\n    \n    engine.initialize_search_methods()\n    \n    # Save the model to a file that can be downloaded\n    model_path = output_dir / \"quran_search_engine.pkl\"\n    try:\n        with open(model_path, 'wb') as f:\n            pickle.dump(engine, f)\n        logger.info(f\"Model saved to {model_path}\")\n        \n        # Get model stats for reporting\n        data_count = len(engine.data) if hasattr(engine, 'data') else 0\n        has_tfidf = engine.vectorizer is not None if hasattr(engine, 'vectorizer') else False\n        has_st = engine.sentence_transformer is not None if hasattr(engine, 'sentence_transformer') else False\n        \n        print(\"\\n\" + \"=\"*50)\n        print(\"MODEL CREATION SUCCESSFUL\")\n        print(\"=\"*50)\n        print(f\"Model saved to: {model_path}\")\n        print(f\"Verses loaded: {data_count}\")\n        print(f\"TF-IDF vectorizer: {'Enabled' if has_tfidf else 'Disabled'}\")\n        print(f\"Semantic search: {'Enabled' if has_st else 'Disabled'}\")\n        print(\"\\nIMPORTANT: Download this file from the Kaggle output\")\n        print(\"=\"*50)\n        \n        return True\n    except Exception as e:\n        logger.error(f\"Failed to save model: {e}\")\n        return False\n\n# Run the model creation\nif __name__ == \"__main__\":\n    create_and_save_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:22:00.556725Z","iopub.execute_input":"2025-04-01T18:22:00.557083Z","iopub.status.idle":"2025-04-01T18:22:12.387211Z","shell.execute_reply.started":"2025-04-01T18:22:00.557053Z","shell.execute_reply":"2025-04-01T18:22:12.386247Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/195 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f9e193ccbad47b7ab345809612828b5"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nMODEL CREATION SUCCESSFUL\n==================================================\nModel saved to: /kaggle/working/model_output/quran_search_engine.pkl\nVerses loaded: 6236\nTF-IDF vectorizer: Enabled\nSemantic search: Enabled\n\nIMPORTANT: Download this file from the Kaggle output\n==================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}